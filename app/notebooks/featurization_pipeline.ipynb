{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e6c87-fd7c-42dd-a6bf-fa3a8420eb68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f490a314-8019-42f6-8d2a-ba7d6509e50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mgenerated new fontManager\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ZenML\n",
    "from zenml import pipeline, step\n",
    "\n",
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# MongoDB\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "# Embedding Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15facd79-012a-4121-b77b-01f2a357c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def load_data_from_mongodb() -> list:\n",
    "    logger.info(\"Loading data from MongoDB...\")\n",
    "    \n",
    "    client = MongoClient('mongodb://rag_mongodb:27017/')\n",
    "    db = client['rag_db']\n",
    "    collection = db['raw_data']\n",
    "    \n",
    "    documents = list(collection.find())\n",
    "    logger.info(f\"Loaded {len(documents)} documents from MongoDB.\")\n",
    "    \n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9d4d33-5b37-4744-b181-2d87e33345dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def categorize_and_preprocess_data(documents: list) -> list:\n",
    "    logger.info(\"Categorizing and preprocessing data...\")\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        content = doc.get('content', '')\n",
    "        file_path = doc.get('path', '')\n",
    "        source = doc.get('source', 'unknown')\n",
    "        url = doc.get('url', '')\n",
    "\n",
    "        # Determine the category\n",
    "        if source == 'github':\n",
    "            if file_path.endswith('.md') or file_path.endswith('.rst'):\n",
    "                category = 'article'\n",
    "                # Additional preprocessing for articles if needed\n",
    "            elif file_path.endswith('.py'):\n",
    "                category = 'code'\n",
    "                if is_valid_python_code(content):\n",
    "                    content = remove_comments_and_docstrings(content)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping invalid Python file: {file_path}\")\n",
    "                    continue\n",
    "            else:\n",
    "                category = 'other'\n",
    "        elif source == 'web':\n",
    "            category = 'article'\n",
    "        elif source == 'youtube':\n",
    "            category = 'article'\n",
    "        else:\n",
    "            category = 'unknown'\n",
    "\n",
    "        processed_data.append({\n",
    "            'url': url,\n",
    "            'path': file_path,\n",
    "            'repository': doc.get('repository', ''),\n",
    "            'branch': doc.get('branch', ''),\n",
    "            'content': content,\n",
    "            'source': source,\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    logger.info(f\"Categorized and processed {len(processed_data)} documents.\")\n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e738c9-b2fb-47a7-83b3-45976f3aa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def is_valid_python_code(source):\n",
    "    \"\"\"\n",
    "    Validate if the source code is likely valid Python.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ast.parse(source)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def remove_comments_and_docstrings(source):\n",
    "    \"\"\"\n",
    "    Remove comments and docstrings from Python source code.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse the source code into an AST\n",
    "        parsed = ast.parse(source)\n",
    "        for node in ast.walk(parsed):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef, ast.Module)):\n",
    "                # Remove docstrings\n",
    "                if node.body and isinstance(node.body[0], ast.Expr):\n",
    "                    if hasattr(node.body[0], 'value') and isinstance(node.body[0].value, ast.Str):\n",
    "                        node.body = node.body[1:]\n",
    "        return ast.unparse(parsed)\n",
    "    except SyntaxError as e:\n",
    "        logger.warning(f\"Syntax error in Python code: {e}\")\n",
    "        return source  # Return the original source if parsing fails\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Unexpected error parsing Python code: {e}\")\n",
    "        return source  # Return the original source if other errors occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55dbd8e-7470-49f8-bab5-4457c71fedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def chunk_data(processed_data: list) -> list:\n",
    "    logger.info(\"Chunking data...\")\n",
    "    \n",
    "    chunked_data = []\n",
    "    max_chunk_size = 500  # Adjust based on your embedding model's max input length\n",
    "    \n",
    "    for doc in processed_data:\n",
    "        content = doc['content']\n",
    "        # Create a unique ID for the document\n",
    "        doc_id = doc.get('path', '') or doc.get('url', '').replace('/', '_')\n",
    "        if not doc_id:\n",
    "            doc_id = f\"{doc['source']}_{len(chunked_data)}\"\n",
    "\n",
    "        # Split content into chunks\n",
    "        content_length = len(content)\n",
    "        chunks = [content[i:i+max_chunk_size] for i in range(0, content_length, max_chunk_size)]\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'doc_id': str(doc_id),\n",
    "                'chunk_id': f\"{str(doc_id)}_{idx}\",\n",
    "                'chunk': chunk,\n",
    "                'metadata': {\n",
    "                    'url': doc.get('url', ''),\n",
    "                    'path': doc.get('path', ''),\n",
    "                    'repository': doc.get('repository', ''),\n",
    "                    'branch': doc.get('branch', ''),\n",
    "                    'source': doc.get('source', ''),\n",
    "                    'category': doc.get('category', '')\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Created {len(chunked_data)} chunks from documents.\")\n",
    "    return chunked_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97457f3-7597-4c2e-aa48-aa42e8e3e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def generate_embeddings(chunked_data: list) -> list:\n",
    "    logger.info(\"Generating embeddings...\")\n",
    "    \n",
    "    if not chunked_data:\n",
    "        logger.warning(\"No data to generate embeddings for!\")\n",
    "        return []\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    batch_size = 32  # Adjust based on your hardware capabilities\n",
    "    \n",
    "    embeddings = []\n",
    "    total_batches = (len(chunked_data) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(chunked_data), batch_size):\n",
    "        batch = chunked_data[i:i+batch_size]\n",
    "        texts = [item['chunk'] for item in batch]\n",
    "        try:\n",
    "            batch_embeddings = model.encode(texts)\n",
    "            for idx, item in enumerate(batch):\n",
    "                item['embedding'] = batch_embeddings[idx].tolist()\n",
    "            embeddings.extend(batch)\n",
    "            logger.info(f\"Processed batch {(i//batch_size)+1}/{total_batches}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings for batch {i//batch_size}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad94001c-e014-4d28-a916-b465404262b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def store_embeddings_in_qdrant(chunked_data: list):\n",
    "    logger.info(\"Storing embeddings in Qdrant...\")\n",
    "    \n",
    "    try:\n",
    "        client = QdrantClient(host='rag_qdrant', port=6333)\n",
    "        \n",
    "        # Define collection parameters\n",
    "        collection_name = 'rag_collection'\n",
    "        vector_size = len(chunked_data[0]['embedding'])\n",
    "        \n",
    "        # Check if collection exists and recreate\n",
    "        try:\n",
    "            client.get_collection(collection_name)\n",
    "            client.delete_collection(collection_name)\n",
    "            logger.info(f\"Deleted existing collection: {collection_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "        )\n",
    "        \n",
    "        # Prepare data for Qdrant\n",
    "        batch_size = 100  # Adjust based on your needs\n",
    "        for i in range(0, len(chunked_data), batch_size):\n",
    "            batch = chunked_data[i:i+batch_size]\n",
    "            points = []\n",
    "            for idx, item in enumerate(batch):\n",
    "                # Generate a positive integer ID using the position in the dataset\n",
    "                point_id = i * batch_size + idx + 1  # Ensures positive, unique IDs starting from 1\n",
    "                \n",
    "                point = PointStruct(\n",
    "                    id=point_id,  # Use the positive integer ID\n",
    "                    vector=item['embedding'],\n",
    "                    payload={\n",
    "                        **item['metadata'],\n",
    "                        'chunk_id': item['chunk_id'],\n",
    "                        'doc_id': item['doc_id'],\n",
    "                        'chunk': item['chunk']\n",
    "                    }\n",
    "                )\n",
    "                points.append(point)\n",
    "            \n",
    "            try:\n",
    "                # Upload batch to Qdrant\n",
    "                client.upsert(\n",
    "                    collection_name=collection_name,\n",
    "                    points=points\n",
    "                )\n",
    "                logger.info(f\"Uploaded batch of {len(points)} embeddings to Qdrant (IDs {points[0].id} to {points[-1].id})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error uploading batch: {str(e)}\")\n",
    "                # Log the first failing point for debugging\n",
    "                if points:\n",
    "                    logger.error(f\"First point in failing batch - ID: {points[0].id}\")\n",
    "                raise\n",
    "        \n",
    "        logger.info(f\"Successfully stored all {len(chunked_data)} embeddings in Qdrant.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error storing embeddings in Qdrant: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74788f49-d81b-475a-8718-ab7a665c9d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, create a global variable to store the results\n",
    "pipeline_results = None\n",
    "\n",
    "@pipeline\n",
    "def featurization_pipeline():\n",
    "    try:\n",
    "        documents = load_data_from_mongodb()\n",
    "        if not documents:\n",
    "            logger.error(\"No documents loaded from MongoDB!\")\n",
    "            return None\n",
    "            \n",
    "        processed_data = categorize_and_preprocess_data(documents)\n",
    "        if not processed_data:\n",
    "            logger.error(\"No documents after preprocessing!\")\n",
    "            return None\n",
    "            \n",
    "        chunked_data = chunk_data(processed_data)\n",
    "        if not chunked_data:\n",
    "            logger.error(\"No chunks created!\")\n",
    "            return None\n",
    "            \n",
    "        chunked_data_with_embeddings = generate_embeddings(chunked_data)\n",
    "        if not chunked_data_with_embeddings:\n",
    "            logger.error(\"No embeddings generated!\")\n",
    "            return None\n",
    "            \n",
    "        store_embeddings_in_qdrant(chunked_data_with_embeddings)\n",
    "        logger.info(\"Pipeline completed successfully!\")\n",
    "        \n",
    "        # Return the results\n",
    "        return chunked_data_with_embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Run the pipeline and store results\n",
    "pipeline_instance = featurization_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916509c9-331b-4915-9cf0-fae96f4bf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test the results\n",
    "def search_qdrant(query_text: str, limit: int = 3):\n",
    "    try:\n",
    "        # Connect to Qdrant\n",
    "        qdrant_client = QdrantClient(host='rag_qdrant', port=6333)\n",
    "        \n",
    "        # Load the same model used in the pipeline\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Generate embedding for the query\n",
    "        query_vector = model.encode(query_text)\n",
    "        \n",
    "        # Ensure the query vector is a list of floats\n",
    "        if not isinstance(query_vector, list):\n",
    "            query_vector = query_vector.tolist()\n",
    "        \n",
    "        # Log the query vector for debugging\n",
    "        logger.debug(f\"Query vector: {query_vector[:10]}...\")  # Log first 10 elements\n",
    "        \n",
    "        # Search\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name='rag_collection',\n",
    "            query_vector=query_vector,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nSearch Results for: '{query_text}'\")\n",
    "        print(\"-\" * 50)\n",
    "        for result in search_results:\n",
    "            print(f\"Score: {result.score:.4f}\")\n",
    "            print(f\"Repository: {result.payload.get('repository')}\")\n",
    "            print(f\"Path: {result.payload.get('path')}\")\n",
    "            print(f\"Category: {result.payload.get('category')}\")\n",
    "            print(f\"Chunk: {result.payload.get('chunk')[:200]}...\")  # Show first 200 chars\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching Qdrant: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "search_qdrant(\"What is ROS?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
