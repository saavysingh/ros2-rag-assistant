{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a56fc3-ec58-454a-bc74-85061e8e9b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620bcd1-3619-402a-919d-bd323dc0549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import gradio as gr\n",
    "from pymongo import MongoClient\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Connect to Qdrant (adjust host/port if different)\n",
    "qdrant_client = QdrantClient(host='rag_qdrant', port=6333)\n",
    "collection_name = 'rag_collection'\n",
    "\n",
    "# Embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7271792b-9962-4ec6-8c9d-adc5d587aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def retrieve_relevant_chunks(query_embedding, top_k=5):\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    retrieved_texts = [result.payload['chunk'] for result in search_results]\n",
    "    return retrieved_texts\n",
    "\n",
    "def call_ollama(prompt, model=\"llama2\"):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"stream\": False  # Request a full response, not streaming\n",
    "        }\n",
    "    }\n",
    "\n",
    "    url = os.getenv(\"OLLAMA_HOST\", \"http://host.docker.internal:11434\")\n",
    "    try:\n",
    "        # Send the POST request\n",
    "        response = requests.post(f\"{url}/api/generate\", json=payload, timeout=30)  # Add timeout\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Handle raw response in case of multiple JSON objects\n",
    "        raw_response = response.text.strip()  # Raw response as text\n",
    "        # print(\"Raw Response:\", raw_response)\n",
    "\n",
    "        # Combine 'response' fields from multiple JSON objects\n",
    "        responses = []\n",
    "        for line in raw_response.splitlines():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'response' in data:\n",
    "                    responses.append(data['response'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Error parsing line as JSON: {e} - Line: {line}\")\n",
    "\n",
    "        # Join all parts of the response\n",
    "        full_response = ''.join(responses)\n",
    "        return full_response if full_response else \"No response generated.\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error calling Ollama API: {e}\")\n",
    "        return \"Sorry, I'm having trouble connecting to the language model.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd9fa74-f1ee-4e1d-bb18-f00c8f8a9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a query.\"\n",
    "    \n",
    "    # Embed the user query\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "\n",
    "    # Retrieve relevant chunks from Qdrant\n",
    "    retrieved_texts = retrieve_relevant_chunks(query_embedding, top_k=5)\n",
    "\n",
    "    # Construct the prompt\n",
    "    context = \"\\n\".join(retrieved_texts)\n",
    "    prompt = f\"You are a helpful ROS2 expert assistant. Use the following context to answer the question. If you cannot find the answer in the context, say so. If providing code, ensure it's well-commented. Context:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "\n",
    "    answer = call_ollama(prompt, model=\"llama2\").strip()\n",
    "    if not answer:\n",
    "        answer = \"I'm not sure how to answer that.\"\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72acfe74-6ec5-48c9-9dee-185548fbb1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625f16a949334d409fbde0938455ef21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://rag_qdrant:6333/collections/rag_collection/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: To navigate to a specific pose in ROS2 using the `navigation` stack, you can use the `NavigateToPoseAction` action server. Here's an example code snippet that demonstrates how to use this action server:\n",
      "```python\n",
      "import rospy\n",
      "from geometry_msgs.msg import PoseStamped\n",
      "\n",
      "# Initialize ROS node\n",
      "rospy.init_node('navigate_to_pose')\n",
      "\n",
      "# Set the goal pose\n",
      "goal = PoseStamped()\n",
      "goal.header.stamp = rospy.Time(10)  # Set the timestamp to a random value\n",
      "goal.position.x = 0.5\n",
      "goal.position.y = 0.5\n",
      "goal.position.z = 0.5\n",
      "goal.orientation = 1.0\n",
      "\n",
      "# Create an action server for navigating to the goal pose\n",
      "server = rospy.ActionServer('navigate_to_pose', ' navigation2::NavigateToPoseAction')\n",
      "\n",
      "try:\n",
      "    # Subscribe to the goal pose topic\n",
      "    rospy.Subscriber('/goal_pose', PoseStamped, server)\n",
      "    \n",
      "    # Wait for a goal pose message\n",
      "    while True:\n",
      "        try:\n",
      "            goal_msg = rospy.wait_for_message('/goal_pose', PoseStamped)\n",
      "            if goal_msg:\n",
      "                server.execute(goal_msg)\n",
      "        except rospy.exceptions.ROSException:\n",
      "            pass\n",
      "except rospy.exceptions.ROSException:\n",
      "    pass\n",
      "```\n",
      "In this code, we first initialize the ROS node and then set the goal pose using a `PoseStamped` message. We then create an action server for navigating to the goal pose using the `NavigateToPoseAction` action server.\n",
      "\n",
      "We then subscribe to the `/goal_pose` topic, where the goal pose message will be published. We use the `wait_for_message` function to wait for a goal pose message, and once we receive one, we execute it using the `execute` method of the action server.\n",
      "\n",
      "Note that you can also specify additional parameters when creating the action server, such as the `orientation` and `speed` parameters, which can be used to customize the navigation behavior.\n",
      "\n",
      "You can also use other action servers provided by the `navigation2` stack, such as `NavigateToCoordinateAction`, `NavigateToVelocityProfileAction`, or `NavigateToAccelerationProfileAction`, depending on your specific requirements.\n"
     ]
    }
   ],
   "source": [
    "test_query = \"How do I navigate to a specific pose in ROS2? Can you provide me with code for this task?\"\n",
    "print(\"\\n Answer:\", generate_response(test_query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66cd4eb-05d2-493d-bb76-8304b8f96de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://localhost:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"ROS2 RAG Assistant\",\n",
    "    description=\"Ask questions about ROS2. The system retrieves relevant info and uses the saavysingh/llama2-rag model to generate an answer.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
