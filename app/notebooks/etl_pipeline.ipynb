{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80d922a-98ed-4e10-af29-22b59dc16ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env at: /app/dotenv.env\n",
      "Load successful: True\n"
     ]
    }
   ],
   "source": [
    "from zenml import pipeline, step\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "env_path = os.path.abspath(os.path.join(os.getcwd(), '../dotenv.env'))\n",
    "print(f\"Looking for .env at: {env_path}\")\n",
    "loaded = load_dotenv(env_path)\n",
    "print(f\"Load successful: {loaded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2abcb6-c558-46c2-91c6-268e3835b86d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /usr/local/lib/python3.9/site-packages (1.51.0)\n",
      "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.9/site-packages (from playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.9/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from pyee<13,>=12->playwright) (4.13.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Playwright Host validation warning: \n",
      "╔══════════════════════════════════════════════════════╗\n",
      "║ Host system is missing dependencies to run browsers. ║\n",
      "║ Please install them with the following command:      ║\n",
      "║                                                      ║\n",
      "║     playwright install-deps                          ║\n",
      "║                                                      ║\n",
      "║ Alternatively, use apt:                              ║\n",
      "║     apt-get install libglib2.0-0\\                    ║\n",
      "║         libnss3\\                                     ║\n",
      "║         libnspr4\\                                    ║\n",
      "║         libdbus-1-3\\                                 ║\n",
      "║         libatk1.0-0\\                                 ║\n",
      "║         libatk-bridge2.0-0\\                          ║\n",
      "║         libcups2\\                                    ║\n",
      "║         libxcomposite1\\                              ║\n",
      "║         libxdamage1\\                                 ║\n",
      "║         libxext6\\                                    ║\n",
      "║         libxfixes3\\                                  ║\n",
      "║         libxrandr2\\                                  ║\n",
      "║         libgbm1\\                                     ║\n",
      "║         libxkbcommon0\\                               ║\n",
      "║         libpango-1.0-0\\                              ║\n",
      "║         libcairo2\\                                   ║\n",
      "║         libasound2\\                                  ║\n",
      "║         libatspi2.0-0                                ║\n",
      "║                                                      ║\n",
      "║ <3 Playwright Team                                   ║\n",
      "╚══════════════════════════════════════════════════════╝\n",
      "    at validateDependenciesLinux (/usr/local/lib/python3.9/site-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
      "    at async Registry._validateHostRequirements (/usr/local/lib/python3.9/site-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
      "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.9/site-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
      "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.9/site-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
      "    at async t.<anonymous> (/usr/local/lib/python3.9/site-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6286574",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def search_youtube_videos(queries: list, max_results_per_query: int = 10) -> list:\n",
    "    YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')\n",
    "    if not YOUTUBE_API_KEY:\n",
    "        raise ValueError(\"YOUTUBE_API_KEY not found in environment.\")\n",
    "\n",
    "    video_ids = []\n",
    "    for query in queries:\n",
    "        params = {\n",
    "            'part': 'snippet',\n",
    "            'type': 'video',\n",
    "            'maxResults': max_results_per_query,\n",
    "            'q': query,\n",
    "            'key': YOUTUBE_API_KEY\n",
    "        }\n",
    "        resp = requests.get('https://www.googleapis.com/youtube/v3/search', params=params)\n",
    "        if resp.status_code == 200:\n",
    "            results = resp.json().get('items', [])\n",
    "            for item in results:\n",
    "                video_id = item['id']['videoId']\n",
    "                video_ids.append(video_id)\n",
    "        else:\n",
    "            print(f\"Failed YouTube search for {query}: {resp.text}\")\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "@step\n",
    "def extract_youtube_data(video_ids: list) -> list:\n",
    "    data = []\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(vid)\n",
    "            full_text = \" \".join([t[\"text\"] for t in transcript])\n",
    "            data.append({\n",
    "                'url': f\"https://www.youtube.com/watch?v={vid}\",\n",
    "                'path': None,\n",
    "                'content': full_text,\n",
    "                'source': 'youtube',\n",
    "                'repository': None,\n",
    "                'branch': None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching transcript for video {vid}: {e}\")\n",
    "            logger.error(f\"Error fetching transcript for video {vid}: {e}\")\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5edc27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "@step\n",
    "def discover_web_pages(root_urls: list, max_pages_per_root: int = 10) -> list:\n",
    "    # Web crawling\n",
    "    all_urls = []\n",
    "\n",
    "    for root_url in root_urls:\n",
    "        try:\n",
    "            logger.info(f\"Fetching root URL: {root_url}\")\n",
    "            response = requests.get(root_url)\n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f\"Failed to fetch {root_url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            parsed_root = urlparse(root_url)\n",
    "            base_domain = parsed_root.netloc\n",
    "\n",
    "            extracted_urls = []\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                full_url = urljoin(root_url, href) \n",
    "                parsed_url = urlparse(full_url)\n",
    "\n",
    "          \n",
    "                if base_domain in parsed_url.netloc and 'text/html' in requests.head(full_url).headers.get('Content-Type', ''):\n",
    "                    extracted_urls.append(full_url)\n",
    "\n",
    "            # Deduplicate and limit results\n",
    "            unique_urls = list(dict.fromkeys(extracted_urls))\n",
    "            limited_urls = unique_urls[:max_pages_per_root]\n",
    "            all_urls.extend(limited_urls)\n",
    "            # print(limited_urls)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error discovering web pages from {root_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "@step\n",
    "def extract_web_data(urls: list) -> list:\n",
    "    data = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            logger.info(f\"Scraping URL: {url}\")\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                logger.warning(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.decompose()\n",
    "\n",
    "            # Extract and clean text content\n",
    "            text = soup.get_text(separator=' ')\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            data.append({\n",
    "                'url': url,\n",
    "                'path': None,\n",
    "                'content': text,\n",
    "                'source': 'web',\n",
    "                'repository': None,\n",
    "                'branch': None\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2717e11-bc6b-4cb5-a173-9585022ef88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def extract_github_data() -> list:\n",
    "    import time\n",
    "    logger.info(\"Starting data extraction from GitHub...\")\n",
    "\n",
    "    GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "    headers = {\n",
    "        'Authorization': f'token {GITHUB_TOKEN}',\n",
    "        'Accept': 'application/vnd.github.v3+json' \n",
    "    }\n",
    "\n",
    "    # List of repositories to scrape\n",
    "    repositories = [\n",
    "        {'owner': 'ros2', 'repo': 'ros2_documentation', 'branch': 'rolling'},\n",
    "        {'owner': 'ros2', 'repo': 'examples', 'branch': 'rolling'},\n",
    "        {'owner': 'ros2', 'repo': 'demos', 'branch': 'rolling'},\n",
    "        {'owner': 'ros2', 'repo': 'rclpy', 'branch': 'rolling'},\n",
    "        {'owner': 'ros2', 'repo': 'rclcpp', 'branch': 'rolling'},\n",
    "        {'owner': 'ros-navigation', 'repo': 'docs.nav2.org', 'branch': 'master'},\n",
    "        {'owner': 'ros-navigation', 'repo': 'navigation2', 'branch': 'main'},\n",
    "        {'owner': 'moveit', 'repo': 'moveit2', 'branch': 'main'},\n",
    "        {'owner': 'gazebosim', 'repo': 'gz-doc', 'branch': 'master'}\n",
    "    ]\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for repo in repositories:\n",
    "        owner = repo['owner']\n",
    "        repo_name = repo['repo']\n",
    "        branch = repo['branch']\n",
    "        logger.info(f\"Fetching files from repository: {owner}/{repo_name}\")\n",
    "\n",
    "        tree_url = f'https://api.github.com/repos/{owner}/{repo_name}/git/trees/{branch}?recursive=1'\n",
    "        response = requests.get(tree_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Failed to fetch tree for {owner}/{repo_name}: {response.text}\")\n",
    "            continue\n",
    "\n",
    "        tree = response.json().get('tree', [])\n",
    "        file_urls = []\n",
    "        for item in tree:\n",
    "            if item['type'] == 'blob' and item['path'].endswith(('.md', '.rst', '.py')):\n",
    "                raw_url = f'https://raw.githubusercontent.com/{owner}/{repo_name}/{branch}/{item[\"path\"]}'\n",
    "                file_urls.append({'url': raw_url, 'path': item['path']})\n",
    "\n",
    "        logger.info(f\"Found {len(file_urls)} files to download in {owner}/{repo_name}\")\n",
    "\n",
    "        # Fetch and store file contents\n",
    "        for file_info in file_urls:\n",
    "            file_url = file_info['url']\n",
    "            file_path = file_info['path']\n",
    "            try:\n",
    "                file_response = requests.get(file_url, headers=headers)\n",
    "                if file_response.status_code == 200:\n",
    "                    content = file_response.text\n",
    "                    data.append({\n",
    "                        'url': file_url,\n",
    "                        'path': file_path,\n",
    "                        'content': content,\n",
    "                        'source': 'github',\n",
    "                        'repository': f'{owner}/{repo_name}',\n",
    "                        'branch': branch\n",
    "                    })\n",
    "                    logger.debug(f\"Fetched {file_url}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Failed to fetch {file_url}: {file_response.status_code}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching {file_url}: {e}\")\n",
    "\n",
    "    logger.info(f\"Extracted {len(data)} files from GitHub.\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b94f5b-5f25-489c-a468-0b0955044a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def transform_data(data: list) -> list:\n",
    "    logger.info(\"Starting data transformation...\")\n",
    "\n",
    "    transformed_data = []\n",
    "\n",
    "    def clean_text(text):\n",
    "        if text.endswith('.py'):\n",
    "            # For Python files, keep the content as is\n",
    "            return text.strip()\n",
    "        text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)  # Remove images\n",
    "        text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)    # Remove links\n",
    "        text = re.sub(r'#.*', '', text)  # Remove headings\n",
    "        # Remove excessive whitespace and newlines\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    for item in data:\n",
    "        content = clean_text(item['content'])\n",
    "        if content:\n",
    "            transformed_item = {\n",
    "                'url': item['url'],\n",
    "                'path': item['path'],\n",
    "                'repository': item['repository'],\n",
    "                'branch': item['branch'],\n",
    "                'content': content,\n",
    "                'source': item['source']\n",
    "            }\n",
    "            transformed_data.append(transformed_item)\n",
    "            logger.debug(f\"Transformed data from {item['url']}\")\n",
    "        else:\n",
    "            logger.warning(f\"No content after cleaning for {item['url']}\")\n",
    "\n",
    "    logger.info(f\"Transformed {len(transformed_data)} items.\")\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf4cb1a-9aef-4449-a376-a8abea9a4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def load_data(transformed_data: list):\n",
    "    logger.info(\"Starting data loading into MongoDB...\")\n",
    "\n",
    "    client = MongoClient('mongodb://rag_mongodb:27017/')\n",
    "    db = client['rag_db']\n",
    "    collection = db['raw_data']\n",
    "\n",
    "    # Insert data into MongoDB\n",
    "    if transformed_data:\n",
    "        collection.insert_many(transformed_data)\n",
    "        logger.info(f\"Inserted {len(transformed_data)} documents into MongoDB\")\n",
    "    else:\n",
    "        logger.warning(\"No data to insert into MongoDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ce5b0-9ec2-408b-b6e5-c29d5963c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def data_collection_pipeline():\n",
    "    # GitHub data\n",
    "    github_data = extract_github_data()\n",
    "    transformed_github_data = transform_data(github_data)\n",
    "    load_data(transformed_github_data)\n",
    "\n",
    "    # YouTube data\n",
    "    youtube_search_queries = [\"ROS2\",\" ROS2 navigation\", \"ROS2 MoveIt2\", \"ROS2 Gazebo simulation\"]\n",
    "    video_ids = search_youtube_videos(youtube_search_queries, max_results_per_query=5)\n",
    "    youtube_data = extract_youtube_data(video_ids)\n",
    "    transformed_youtube_data = transform_data(youtube_data)\n",
    "    load_data(transformed_youtube_data)\n",
    "\n",
    "    # Web data\n",
    "    root_urls = [\n",
    "        \"https://docs.nav2.org/\",\n",
    "        \"https://moveit.picknik.ai/main/\",\n",
    "        \"https://gazebosim.org/docs/all/tutorials/\",\n",
    "        \"https://docs.ros.org/en/foxy/Tutorials/\"\n",
    "    ]\n",
    "    discovered_urls = discover_web_pages(root_urls, max_pages_per_root=100)\n",
    "    web_data = extract_web_data(discovered_urls)\n",
    "    transformed_web_data = transform_data(web_data)\n",
    "    load_data(transformed_web_data)\n",
    "\n",
    "pipeline_instance = data_collection_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7357d8-b30e-434e-bd3d-39467314ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb://rag_mongodb:27017/')\n",
    "db = client['rag_db']\n",
    "collection = db['raw_data']\n",
    "\n",
    "doc_count = collection.count_documents({})\n",
    "print(f\"Total documents in raw_data collection: {doc_count}\")\n",
    "\n",
    "urls = collection.distinct('url')\n",
    "print(\"URLs ingested:\")\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
